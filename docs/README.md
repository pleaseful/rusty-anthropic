## Rusty Anthropic SDK - Function Documentation

### Messages API

#### Create a Message

**Functionality:**  
Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation. Supports both single queries and stateless multi-turn conversations.

**Method:**  
POST `/v1/messages`

**Parameters:**

- **model (string, required):**  
  The model that will complete your prompt.

- **messages (Vec<Value>, required):**  
  Input messages. The models operate on alternating user and assistant conversational turns. Specify prior conversational turns with this parameter, and the model generates the next message in the conversation.

- **max_tokens (Option<u64>):**  
  The maximum number of tokens to generate before stopping.

- **temperature (Option<f64>):**  
  Amount of randomness injected into the response. Default is 1.0.

- **stop_sequences (Option<Vec<String>>):**  
  Custom text sequences that will cause the model to stop generating.

- **stream (Option<bool>):**  
  Whether to incrementally stream the response using server-sent events.

**Response:**

- **id (string):**  
  Unique object identifier.

- **type (enum<string>):**  
  Object type. Always "message".

- **role (enum<string>):**  
  Conversational role of the generated message. Always "assistant".

- **content (Vec<Value>):**  
  Content generated by the model, typically an array of content blocks.

**Usage Example:**

```rust
let request = MessageRequest::new(model, messages)
    .max_tokens(1024)
    .temperature(1.0);
let response = messages_api.create(request).await?;
```

### Text Completions API

#### Create a Text Completion

**Functionality:**  
Generate a text completion based on a provided prompt, allowing for control over various aspects of the completion process.

**Method:**  
POST `/v1/complete`

**Parameters:**

- **model (string, required):**  
  The model to be used for the text completion.

- **prompt (string, required):**  
  Initial prompt for the text completion.

- **max_tokens_to_sample (Option<u64>):**  
  Maximum number of tokens to generate before stopping.

- **stop_sequences (Option<Vec<String>>):**  
  Custom text sequences that will cause the model to stop generating.

- **temperature (Option<f64>):**  
  Amount of randomness injected into the response. Default is 1.0.

- **top_p (Option<f64>):**  
  Use nucleus sampling.

- **top_k (Option<u64>):**  
  Only sample from the top K options for each subsequent token.

**Response:**

- **completion (string):**  
  The resulting completion up to and excluding the stop sequences.

**Usage Example:**

```rust
let request = TextCompletionRequest::new(model, prompt)
    .max_tokens_to_sample(150)
    .temperature(0.7);
let response = text_completions_api.create(request).await?;
```

### Embeddings API

#### Create Embeddings

**Functionality:**  
Generate embeddings for a list of input texts, useful for tasks like search, recommendations, and anomaly detection.

**Method:**  
POST `/v1/embeddings`

**Parameters:**

- **inputs (Vec<String>, required):**  
  List of texts to create embeddings for.

- **model (string, required):**  
  The model to generate embeddings.

- **input_type (Option<String>):**  
  Type of the input text (e.g., query, document).

- **truncation (Option<bool>):**  
  Whether to truncate the input texts to fit within the context length.

- **encoding_format (Option<String>):**  
  Format in which the embeddings are encoded.

**Response:**

- **embeddings (Vec<Vec<f64>>):**  
  The resulting embeddings for each input text.

**Usage Example:**

```rust
let request = EmbeddingsRequest::new(model, inputs)
    .input_type("document".to_string());
let response = embeddings_api.create(request).await?;
```

### Utilities

#### Utility Functions

- **strings_to_json_array(strings: &[String]) -> Value:**  
  Convert an array of strings to a JSON array.

- **insert_optional_param(params: &mut Map<String, Value>, key: &str, value: Option<impl Into<Value>>):**  
  Handle optional parameters for API requests.